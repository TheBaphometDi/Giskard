from giskard_question_generation import run_question_generation
from giskard_evaluation import run_evaluation
from gemini_answer_generation import run_answer_generation

def main():
    print("=" * 60)
    print("–ü–û–õ–ù–´–ô –†–ê–ë–û–ß–ò–ô –ü–†–û–¶–ï–°–°: GEMINI (–≤—ã–±–æ—Ä —Ç–µ–∫—Å—Ç–∞ + –æ—Ç–≤–µ—Ç—ã) + GISCARD (–≤–æ–ø—Ä–æ—Å—ã + –æ—Ü–µ–Ω–∫–∞)")
    print("=" * 60)
    
    print("\n1Ô∏è‚É£ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ Giskard (—Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º –æ—Ç—Ä—ã–≤–∫–∞ —á–µ—Ä–µ–∑ Gemini)...")
    result = run_question_generation(return_data=True)
    if result and len(result) == 2:
        questions, excerpt = result
        if questions and excerpt:
            print(f"\n‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤ –æ—Ç Giskard")
            print(f"‚úÖ –û—Ç—Ä—ã–≤–æ–∫ –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ Gemini (–¥–ª–∏–Ω–∞: {len(excerpt)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            print("\n2Ô∏è‚É£ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤...")
            model_answers = run_answer_generation(questions, excerpt)
            
            if model_answers:
                print(f"\n‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(model_answers)} –æ—Ç–≤–µ—Ç–æ–≤")
                
                print("\n3Ô∏è‚É£ –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤...")
                evaluation_results = run_evaluation(questions, excerpt, model_answers)
                
                if evaluation_results:
                    print("\nüìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
                    print(f"   –í–æ–ø—Ä–æ—Å–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ (Giskard): {evaluation_results.get('total_questions', 0)}")
                    print(f"   –û—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—É—á–µ–Ω–æ: {len(model_answers)}")
                    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {evaluation_results.get('accuracy', 0):.2%}")
                    print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {evaluation_results.get('success_rate', 0):.1f}%")
                    
                    if 'automatic_metrics' in evaluation_results:
                        auto_metrics = evaluation_results['automatic_metrics']
                        print(f"\nü§ñ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ú–ï–¢–†–ò–ö–ò GISKARD:")
                        print(f"   - –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {auto_metrics.get('correct_answers', 0)}/{auto_metrics.get('total_questions', 0)}")
                        print(f"   - –¢–æ—á–Ω–æ—Å—Ç—å: {auto_metrics.get('accuracy', 0):.2%}")
                        print(f"   - –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {auto_metrics.get('success_rate', 0):.1f}%")
                    
                    if 'evaluation_results' in evaluation_results:
                        print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ (–ø–µ—Ä–≤—ã–µ 5 –≤–æ–ø—Ä–æ—Å–æ–≤):")
                        for i, result in enumerate(evaluation_results['evaluation_results'][:5]):
                            print(f"   –í–æ–ø—Ä–æ—Å {result['question_id']+1}: {'‚úÖ' if result.get('correctness', False) else '‚ùå'}")
                            print(f"      –í–æ–ø—Ä–æ—Å (Giskard): {result['question'][:60]}...")
                            print(f"      –û—Ç–≤–µ—Ç: {result['gemini_answer'][:60]}...")
                            print()
                        
                        if len(evaluation_results['evaluation_results']) > 5:
                            print(f"   ... –∏ –µ—â–µ {len(evaluation_results['evaluation_results']) - 5} –≤–æ–ø—Ä–æ—Å–æ–≤")
                else:
                    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –æ—Ç–≤–µ—Ç—ã")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –æ—Ç Gemini")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã")

if __name__ == "__main__":
    main()
