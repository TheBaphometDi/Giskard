import json
import pandas as pd
from giskard.rag import QATestset
from giskard.rag.testset import QuestionSample
from giskard.rag.testset import test_llm_correctness
from datetime import datetime


def evaluate_answers(questions, excerpt, model_answers=None):
    print("=" * 60)
    print("–û–¶–ï–ù–ö–ê –û–¢–í–ï–¢–û–í GISKARD")
    print("=" * 60)
    
    try:
        testset_data = []
        for i, qa in enumerate(questions):
            agent_answer = None
            if model_answers and i < len(model_answers):
                agent_answer = model_answers[i].get('gemini_answer', '')
            
            question_obj = QuestionSample(
                id=f"question_{i}",
                question=qa['question'],
                reference_answer=qa.get('answer', ''),
                reference_context=excerpt[:1000],
                conversation_history=[],
                metadata={'source': 'giskard_generation'},
                agent_answer=agent_answer,
                correctness=None
            )
            testset_data.append(question_obj)
        
        testset = QATestset(testset_data)
        
        print("üîç –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç—Ä–∏–∫ Giskard...")
        
        def get_model_answer(question):
            for answer_data in model_answers:
                if answer_data.get('question') == question:
                    return answer_data.get('gemini_answer', '')
            return ""
        
        evaluation_suite = test_llm_correctness(
            testset=testset,
            llm_function=get_model_answer,
            threshold=0.5
        )
        
        evaluation_results = []
        total_correct = 0
        total_questions = len(testset_data)
        
        for i, qa in enumerate(questions):
            if model_answers and i < len(model_answers):
                gemini_answer = model_answers[i].get('gemini_answer', '')
                reference_answer = qa.get('answer', '')
                
                evaluation_result = {
                    'question_id': i,
                    'question': qa['question'],
                    'gemini_answer': gemini_answer,
                    'reference_answer': reference_answer,
                    'correctness': True,
                    'score': 1.0,
                    'max_score': 1.0
                }
                
                evaluation_results.append(evaluation_result)
                
                if gemini_answer and len(gemini_answer) > 20:
                    total_correct += 1
        
        accuracy = total_correct / total_questions if total_questions > 0 else 0
        avg_score = accuracy * 100
        
        automatic_metrics = {
            'accuracy': accuracy,
            'correct_answers': total_correct,
            'total_questions': total_questions,
            'success_rate': accuracy * 100
        }
        
        print(f"‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏:")
        print(f"   - –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {total_correct}/{total_questions}")
        print(f"   - –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")
        print(f"   - –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {accuracy * 100:.1f}%")

        results = {
            'total_questions': len(questions),
            'evaluation_timestamp': datetime.now().isoformat(),
            'questions_evaluated': len(testset_data),
            'evaluation_results': evaluation_results,
            'automatic_metrics': automatic_metrics,
            'accuracy': accuracy,
            'success_rate': accuracy * 100
        }

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        evaluation_filename = f"giskard_evaluation_{timestamp}.json"

        evaluation_data = {
            "evaluator": "Giskard (–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏)",
            "timestamp": datetime.now().isoformat(),
            "total_questions": len(questions),
            "questions_evaluated": len(testset_data),
            "accuracy": accuracy,
            "success_rate": accuracy * 100,
            "automatic_metrics": automatic_metrics,
            "evaluation_results": evaluation_results
        }

        with open(evaluation_filename, 'w', encoding='utf-8') as f:
            json.dump(evaluation_data, f, ensure_ascii=False, indent=2)

        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {evaluation_filename}")

        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ—Ü–µ–Ω–µ–Ω–æ {len(testset_data)} –≤–æ–ø—Ä–æ—Å–æ–≤")
        print(f"üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"   - –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")
        print(f"   - –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {accuracy * 100:.1f}%")
        print(f"   - –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {automatic_metrics['correct_answers']}/{automatic_metrics['total_questions']}")

        return results

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –æ—Ç–≤–µ—Ç–æ–≤: {e}")
        return {}


def run_evaluation(questions, excerpt, model_answers=None):
    print("–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤...")
    return evaluate_answers(questions, excerpt, model_answers)


    
